// starting a zookeeper server
> bin/zookeeper-server-start.sh config/zookeeper.properties


//starting a kafka server
> bin/kafka-server-start.sh config/server.properties


MAIN WORK STARTS FROM HERE

//create a Topic named "test"; we can use any name
>> bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test


//now use list Topic command to get the list of topics created in red color
>> bin/kafka-topics.sh --list --bootstrap-server localhost:9092


// Run the PRODUCER, that will take messages into console to send the server

>> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
 ...Type a message 
....Type another message


// Start a CONSUMER - dumps out standard messages to standard output

>> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
Type a message 
Type another message


//Set up MULTI-BROKER CLUSTER (not preferred if work is with only 1 node)
basically it means expanding cluster by increasing nodes
(here we are expanding our cluster to 3 nodes)
create two configurations for each of the brokers/servers

Navigate to the KAFKA FOLDER using cd in terminal and from there type these 2 commands 

cp config/server.properties config/server-1.properties
cp config/server.properties config/server-2.properties

Now EDIT the two files and set the following properties by going into //CONFIG folder in Kafka folder and then select each one server-1.properties and server-2.properties 

   config/server-1.properties:
    	broker.id=1
    	listeners=PLAINTEXT://:9093
    	log.dirs=/tmp/kafka-logs-1

do the same ---->       config/server-2.properties:
   		 	broker.id=2
   		 	listeners=PLAINTEXT://:9094
    			log.dirs=/tmp/kafka-logs-2


// we already have zookeeper and our single node will be started , so now we just need to start the 2 new nodes

>> bin/kafka-server-start.sh config/server-1.properties &
>> bin/kafka-server-start.sh config/server-2.properties &


// now we create a new Topic of replication factor 3

>> bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic new-topic-name


// since now we have 3 nodes, how to know which broker is doing what ?
// so lets describe topics using "describe command"
>> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic new-topic-name

output >> Topic:new-topic-name   PartitionCount:1    ReplicationFactor:3 Configs:
    Topic: my-replicated-topic  Partition: 0    Leader: 1   Replicas: 1,2,0 Isr: 1,2,0

 // ** leader -> node responsible for all reads and writes for given partition
// replicas -> list of nodes that replicate the log for this partition
// isr (in-sync) replicas -> subset of replicas list 

To know where our original/first topic is we can use the same command

>> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic demoTopic
output >>Topic:test  PartitionCount:1    ReplicationFactor:1 Configs:
    Topic: test Partition: 0    Leader: 0   Replicas: 0 Isr: 0

As expected, the original topic has no replicas and is on server 0 (only server when we created our cluster)

//lets do the same thing again PUBLISH and CONSUME few more messages

>>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic new-topic-name
type new message 1
type new message 2

>>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic new-topic-name
returns new message 1
returns new message 2

// lets test our fault tolerance , Broker 1 was acting as leader 
so lets kill it

>> ps aux | grep server-1.properties

output >>7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...

>>  kill -9 proccesid(pid)   // use top to see details of process

// now lets describe again 

>> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic new-topic-name
Topic: new-topic-name   PartitionCount:1    ReplicationFactor:3 Configs:
    Topic: my-replicated-topic  Partition: 0    Leader: 2   Replicas: 1,2,0 Isr: 2,0

// even though the LEADER is killed, but the messages are still available for consumption 

>> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic new-topic-name
...
my test message 1
my test message 2
Ctrl+C -> to come out of that


// Connecting Kafka to Spark Stream
// create some seed data to test with

>> echo "type any content" > filename.extention

  example: echo "writing\ncontent" > abc.txt

// Now we'll start 2 connectors running in standalone mode by taking three parameters {connect-standalone.properties, connect-source-file.properties, connect-source-sink.properties}

>> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties


Once the kafka connect process starts, the source connector shpuld start reading from "abc.txt" and producing them to topic "connect-test" and the sink connector should start reading msgs from topic "connect-test" and write them to file "test.sink.txt" 
//Now to see if data has been delivered to entire pipeline by examining contents of output file:

>> more test.sink.txt
output : writing 
         content

// since the data is being stored in kafka topic "connect-test" we can also run a console consumer to see data present in topic

>> > bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning

...output

// connectors keep adding the data, so we can add data to file and see it move through pipeline

>> echo Another text line writing>> abc.txt


//NEXT TOPIC ---> Using KAFKA STREAMS TO PROCESS DATA
